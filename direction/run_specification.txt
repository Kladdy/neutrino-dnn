run1:
    In this run, 5 different models are tested with varying levels of learning rates for the Adam optimizer
    The different values for the learning are are 
        run1.1: 0.001 
        run1.2: 0.0005
        run1.3: 0.0001  
        run1.4: 0.00005  
        run1.5: 0.00001   

    With the following python scripts:
        python T13.py run1.1 0.001
        python T13.py run1.2 0.0005
        python T13.py run1.3 0.0001
        python T13.py run1.4 0.00005
        python T13.py run1.5 0.00001

run2: 
    The goal of this run is to clean up and automate as much of the code base as possible

run3:
    This run aims to see what effect the inital Conv2D layers have on the model training.
    There are 5 different models, each with 1, 2, 3, 4 or 5 inital Conv2D layers.
    Ran with the following python scripts:
        python training.py 3.1
        python training.py 3.2
        python training.py 3.3
        python training.py 3.4
        python training.py 3.5

run4:
    This run, based on base_v2, aims to test how many dense layers should be used after the Conv2D layers.
    There are 4 different models, each with 1, 2, 3 or 4 dense layers
    Ran with the following python scripts:
        python training.py 4.1
        python training.py 4.2
        python training.py 4.3
        python training.py 4.4

        conda activate tf-gpu
        python training.py 4.1

run5:
    This run, based on base_v2, aims to test if "relu", "tanh" or "elu" function fits the best
        python training.py 5.1
        python training.py 5.2
        python training.py 5.3

run6:
    This run, based on base_v3, aims to test out batch normalization (BN).
    run6.1 has no BN, run6.2 has BN before activation fcns, run 6.3 has BN after activation fcns
    run6.4 has BN only between Conv2D and dense layers
    run6.5 has BN only between each Conv2D layer, after activation function
    run6.6 has BN only between dense layers, after activation function
    https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/

        python training.py 6.1
        python training.py 6.2
        python training.py 6.3
        python training.py 6.4
        python training.py 6.5
        python training.py 6.6

        conda activate tf-gpu
        python training.py 6.X

run7:
    This run, based on base_v3, aims to test of a "relu" activation function is good 
    before the final dense output layer. run7.1 does not have it, run7.2 does have it
        python training.py 7.1
        python training.py 7.2

        conda activate tf-gpu
        python training.py 7.X

run8:
    This run, based on base_v4, aims to test learning rates and loss functions:
        run8.1:
            learning_rate = 0.0005
            loss_function = "mean_squared_error"
        run8.2:
            learning_rate = 0.0001
            loss_function = "mean_squared_error"
        run8.3:
            learning_rate = 0.00005
            loss_function = "mean_squared_error"
        run8.4:
            learning_rate = 0.0005
            loss_function = "mean_absolute_error"
        run8.5:
            learning_rate = 0.0001
            loss_function = "mean_absolute_error"
        run8.6:
            learning_rate = 0.00005
            loss_function = "mean_absolute_error"

run9:
    Base: base_v5
    Aim: Get a baseline for base_v5

run10: 
    Base: base_v5
    Aim: Test amount of 1-stride Conv2D layers before the 2 stride
    Config:
        run10.1: 0 layers
        run10.2: 1 layer
        run10.3: 2 layers
        run10.4: 3 layers

    sleep 2m && python training.py 12.X

run11: 
    Base: base_v5
    Aim: Test changing the last X Conv2D layers with stride 2 to stride 1
    Config:
        run11.1: 0 layers
        run11.2: 1 layer
        run11.3: 2 layers

run12: 
    Base: base_v5
    Aim: Test different filter amounts for Conv2D layers
    Config:
        run12.1: 10
        run12.2: 50
        run12.3: 100
        run12.4: 150
        run12.5: 200

run13:
    Base: base_v6
    Aim: The great loss function test
    Config:
        run13.1: mean_squared_error
        run13.2: mean_absolute_error
        run13.3: mean_absolute_percentage_error
        run13.4: mean_squared_logarithmic_error
        run13.5: cosine_similarity
        run13.6: huber
        run13.7: log_cosh

run14: 
    Base: base_v6
    Aim: Investigate the performance dependence on filter size
    Config:
        run14.1: 3
        run14.2: 7
        run14.3: 10
        run14.4: 15

run15: 
    Base: base_v6
    Aim: Test performance over amount of files to train on
    Config:
        run15.1: 
        run15.2:
        run15.3: 
        run15.4: 


base_v4:
    Add progress from run6 and run7, add Weights and Biases support


base_v5:
    Progress from run8 was added, and change quantile calculation to use quantile_1d from radiotools

base_v6:
    Based on: run12
    Comments: Additional logging to wand (params count etc) was added. 
              Also, train on 1/3 of data instead of 1/5 of data each epoch