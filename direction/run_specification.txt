run1:
    In this run, 5 different models are tested with varying levels of learning rates for the Adam optimizer
    The different values for the learning are are 
        run1.1: 0.001 
        run1.2: 0.0005
        run1.3: 0.0001  
        run1.4: 0.00005  
        run1.5: 0.00001   

    With the following python scripts:
        python T13.py run1.1 0.001
        python T13.py run1.2 0.0005
        python T13.py run1.3 0.0001
        python T13.py run1.4 0.00005
        python T13.py run1.5 0.00001

run2: 
    The goal of this run is to clean up and automate as much of the code base as possible

run3:
    This run aims to see what effect the inital Conv2D layers have on the model training.
    There are 5 different models, each with 1, 2, 3, 4 or 5 inital Conv2D layers.
    Ran with the following python scripts:
        python training.py 3.1
        python training.py 3.2
        python training.py 3.3
        python training.py 3.4
        python training.py 3.5

run4:
    This run, based on base_v2, aims to test how many dense layers should be used after the Conv2D layers.
    There are 4 different models, each with 1, 2, 3 or 4 dense layers
    Ran with the following python scripts:
        python training.py 4.1
        python training.py 4.2
        python training.py 4.3
        python training.py 4.4

        conda activate tf-gpu
        python training.py 4.1

run5:
    This run, based on base_v2, aims to test if "relu", "tanh" or "elu" function fits the best
        python training.py 5.1
        python training.py 5.2
        python training.py 5.3

        conda activate tf-gpu
        python training.py 5.1