run1:
    In this run, 5 different models are tested with varying levels of learning rates for the Adam optimizer
    The different values for the learning are are 
        run1.1: 0.001 
        run1.2: 0.0005
        run1.3: 0.0001  
        run1.4: 0.00005  
        run1.5: 0.00001   

    With the following python scripts:
        python T13.py run1.1 0.001
        python T13.py run1.2 0.0005
        python T13.py run1.3 0.0001
        python T13.py run1.4 0.00005
        python T13.py run1.5 0.00001

run2: 
    The goal of this run is to clean up and automate as much of the code base as possible

run3:
    This run aims to see what effect the inital Conv2D layers have on the model training.
    There are 5 different models, each with 1, 2, 3, 4 or 5 inital Conv2D layers.
    Ran with the following python scripts:
        python training.py 3.1
        python training.py 3.2
        python training.py 3.3
        python training.py 3.4
        python training.py 3.5

run4:
    This run, based on base_v2, aims to test how many dense layers should be used after the Conv2D layers.
    There are 4 different models, each with 1, 2, 3 or 4 dense layers
    Ran with the following python scripts:
        python training.py 4.1
        python training.py 4.2
        python training.py 4.3
        python training.py 4.4

        conda activate tf-gpu
        python training.py 4.1

run5:
    This run, based on base_v2, aims to test if "relu", "tanh" or "elu" function fits the best
        python training.py 5.1
        python training.py 5.2
        python training.py 5.3

run6:
    This run, based on base_v3, aims to test out batch normalization (BN).
    run6.1 has no BN, run6.2 has BN before activation fcns, run 6.3 has BN after activation fcns
    run6.4 has BN only between Conv2D and dense layers
    run6.5 has BN only between each Conv2D layer, after activation function
    run6.6 has BN only between dense layers, after activation function
    https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/

        python training.py 6.1
        python training.py 6.2
        python training.py 6.3
        python training.py 6.4
        python training.py 6.5
        python training.py 6.6

        conda activate tf-gpu
        python training.py 6.X

run7:
    This run, based on base_v3, aims to test of a "relu" activation function is good 
    before the final dense output layer. run7.1 does not have it, run7.2 does have it
        python training.py 7.1
        python training.py 7.2

        conda activate tf-gpu
        python training.py 7.X








base_v4:
    Add progress from run6 and run7, add Weights and Biases support