run1:
    In this run, 5 different models are tested with varying levels of learning rates for the Adam optimizer
    The different values for the learning are are 
        run1.1: 0.001 
        run1.2: 0.0005
        run1.3: 0.0001  
        run1.4: 0.00005  
        run1.5: 0.00001   

run2: 
    The goal of this run is to clean up and automate as much of the code base as possible

run3:
    This run aims to see what effect the inital Conv2D layers have on the model training.
    There are 5 different models, each with 1, 2, 3, 4 or 5 inital Conv2D layers.
    Ran with the following python scripts:

run4:
    This run, based on base_v2, aims to test how many dense layers should be used after the Conv2D layers.
    There are 4 different models, each with 1, 2, 3 or 4 dense layers
    Ran with the following python scripts:

        conda activate tf-gpu
        python training.py 4.1

run5:
    This run, based on base_v2, aims to test if "relu", "tanh" or "elu" function fits the best

run6:
    This run, based on base_v3, aims to test out batch normalization (BN).
    run6.1 has no BN, run6.2 has BN before activation fcns, run 6.3 has BN after activation fcns
    run6.4 has BN only between Conv2D and dense layers
    run6.5 has BN only between each Conv2D layer, after activation function
    run6.6 has BN only between dense layers, after activation function
    https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/


run7:
    This run, based on base_v3, aims to test of a "relu" activation function is good 
    before the final dense output layer. run7.1 does not have it, run7.2 does have it

        conda activate tf-gpu
        python training.py 7.X

run8:
    This run, based on base_v4, aims to test learning rates and loss functions:
        run8.1:
            learning_rate = 0.0005
            loss_function = "mean_squared_error"
        run8.2:
            learning_rate = 0.0001
            loss_function = "mean_squared_error"
        run8.3:
            learning_rate = 0.00005
            loss_function = "mean_squared_error"
        run8.4:
            learning_rate = 0.0005
            loss_function = "mean_absolute_error"
        run8.5:
            learning_rate = 0.0001
            loss_function = "mean_absolute_error"
        run8.6:
            learning_rate = 0.00005
            loss_function = "mean_absolute_error"

run9:
    Base: base_v5
    Aim: Get a baseline for base_v5

run10: 
    Base: base_v5
    Aim: Test amount of 1-stride Conv2D layers before the 2 stride
    Config:
        run10.1: 0 layers
        run10.2: 1 layer
        run10.3: 2 layers
        run10.4: 3 layers

    sleep 2m && python training.py 12.X

run11: 
    Base: base_v5
    Aim: Test changing the last X Conv2D layers with stride 2 to stride 1
    Config:
        run11.1: 0 layers
        run11.2: 1 layer
        run11.3: 2 layers

run12: 
    Base: base_v5
    Aim: Test different filter amounts for Conv2D layers
    Config:
        run12.1: 10
        run12.2: 50
        run12.3: 100
        run12.4: 150
        run12.5: 200

run13:
    Base: base_v6
    Aim: The great loss function test
    Config:
        run13.1: mean_squared_error
        run13.2: mean_absolute_error
        run13.3: mean_absolute_percentage_error
        run13.4: mean_squared_logarithmic_errorcd
        run13.5: cosine_similarity
        run13.6: huber    FAILED
        run13.7: log_cosh FAILED
        run13.8: log_cosh
        run13.9: huber with delta = 1
        run13.10: huber with delta = 0.5

run14: 
    Base: base_v6
    Aim: Investigate the performance dependence on filter size
    Config:
        run14.1: 3
        run14.2: 7
        run14.3: 10
        run14.4: 15
        run14.5: 20 FAILED (layer becomes too small!)

run15: 
    Base: base_v6
    Aim: Test performance over amount of files to train on
    Config:
        run15.1: n_files = 80, n_train = 60, n_val = 12, n_test = 1 
        run15.2: n_files = 40, n_train = 30, n_val = 6, n_test = 1 
        run15.3: n_files = 20, n_train = 15, n_val = 3, n_test = 1
        
    
run16: FAILED! Too big dense layer network...  Maybe reduce?
    Base: base_v7
    Aim: Test stride length dependence
    Config:
        run16.1: stride length 1
        run16.2: stride length 2
        run16.3: stride length 3
        run16.4: stride length 4

run17: 
    Base: base_v7
    Aim: Test stride length dependence on last Conv2D layer
    Config:
        run17.1: stride length 1
        run17.2: stride length 2
        run17.3: stride length 3
        run17.4: stride length 4

run18: 
    Base: base_v7
    Aim: Test stride length dependence on last Conv2D layer.
            Same as run17, but with one less Conv2D layer before
    Config:
        run17.1: stride length 1
        run17.2: stride length 2
        run17.3: stride length 3
        run17.4: stride length 4

run19:
    Base: base_v7
    Aim: Test some filter size configs
    Config:

run20:
    Base: base_v7
    Aim: Run 4 runs and see if result is super different every time

run21:
    Base: base_v7
    Aim: Test channels_first for Conv2D layers and test some pooling

run22:
    Base: base_v8
    Aim: Test different FC layer models

run23:
    Base: base_v8
    Aim: Test l2-normalization after last dense layer

run24:
    Base: base_v9
    Aim: Run many tries of same model to see spread

run25:
    Base: base_v9
    Aim: Test different dense layers

run26:
    Base: base_v9 + progrss from run25
    Aim: Do 3 runs with BatchNormalization, 3 runs without

run27:
    Base: base_v10
    Aim: Dense layer tests

run28:
    Base: base_v10
    Aim: Conv2D layer tests

run29:
    Base: base_v10
    Aim: Test batch size

run30: 
    Base: base_v11
    Aim: Test https://datascience.stackexchange.com/a/26218

run31:
    Base: base_v11
    Aim: Test and see if i can load less data into memory, see if it inpacts performance of model training

run32:
    Base: base_v11
    Aim: Test amount of prefetch data

run33:
    Base: base_v11
    Aim: Test smaller filter sizes in Conv2D layers

run34:
    Base: base_v11
    Aim: 

run35:
    Base: base_v11
    Aim: 

run36:
    Base: base_v11
    Aim: 

run37:
    Base: N\A
    Aim: Complete rehaul

run38:
    Base: run37
    Aim:

run39:
    Base: base_v11
    Aim: Test if normalizing data helps

base_v4:
    Add progress from run6 and run7, add Weights and Biases support

base_v5:
    Progress from run8 was added, and change quantile calculation to use quantile_1d from radiotools

base_v6:
    Based on: run12
    Comments: Additional logging to wand (params count etc) was added. 
              Also, train on 1/3 of data instead of 1/5 of data each epoch

base_v7:
    Based on: run15
    Comments: Testing on more files (3 files instead of 1)

base_v8:
    Based on: run19
    Comments: Nothing really changed

base_v9:
    Based on: run23
    Comments: Add normalization to output, change dense layers, run over all files each epoch

base_v10:
    Based on: run26
    Comments: BatchNormalization was good

base_v11:
    Based on: run29
    Comments: Add min_delta to EarlyStopping

base_v12:
    Based on: run38
    Comments: Complete Conv2D revamp

base_v12:
    Based on: base_v12 ONLY!
    Comments: Add logging of reoslution histogram images to wandb